---
title: "twitter experiment power analysis"
output: 
    html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(here)
library(haven)
library(tidyverse)
library(stargazer)
library(sandwich)
library(sylvan.utils)
library(stringr)
```

# Simple power analysis

Set hypothesized effect to 0.1SD, which is the smallest significant size of effect reported in:

* Broockman and Kalla, 2022
* Bail 2018 reports ITT estimates of around 0.15SD

```{r}
possible.ns <- seq(from = 100, to = 3000, by = 100) # The sample sizes we'll be considering
stopifnot(all((possible.ns %% 2) == 0)) ## require even number of experimental pool
powers <- rep(NA, length(possible.ns)) # Empty object to collect simulation estimates
alpha <- 0.05 # Standard significance level
sims <- 500 # Number of simulations to conduct for each N
#### Outer loop to vary the number of subjects ####
for (j in 1:length(possible.ns)) {
    N <- possible.ns[j] # Pick the jth value for N
    significant.experiments <- rep(NA, sims) # Empty object to count significant experiments

    #### Inner loop to conduct experiments "sims" times over for each N ####
    Y0 <- rnorm(n = N, mean = 0, sd = 1) # control potential outcome
    tau <- 0.1 # Hypothesized treatment effect
    Y1 <- Y0 + tau # treatment potential outcome
    for (i in 1:sims) {
        Z.sim <- sample(rep(c(0, 1), N / 2)) ## Do a random assignment ensuring equal sized groups
        Y.sim <- Y1 * Z.sim + Y0 * (1 - Z.sim) # Reveal outcomes according to assignment
        fit.sim <- lm(Y.sim ~ Z.sim) # Do analysis (Simple regression)
        p.value <- summary(fit.sim)$coefficients[2, 4] # Extract p-values
        significant.experiments[i] <- (p.value <= alpha) # Determine significance according to p <= 0.05
    }
    powers[j] <- mean(significant.experiments) # store average success rate (power) for each N
}
df <- data.frame(powers = powers, ns = possible.ns)
ggplot(data = df, aes(x = ns, y = powers)) +
    geom_point()
```

This simulation suggests we would need a total N of around 3000 to get power for alpha = 0.05 and beta = 0.8.

However, we can probably do better by using covariates controls.

Bail uses the following controls:
```{r}
bail_controls <- c(
    "birth_year",
    "family_income",
    "education",
    "gender",
    "ideo_homogeneity_offline",
    "northeast",
    "north_central",
    "south",
    "percent_co_party",
    "strong_partisan",
    "political_wave_1",
    "freq_twitter_wave_1",
    "party_id_wave_1"
)
load(here("analysis/data/Bail et al PNAS 2018.Rdata"))
# loads as twitter_data
```
```{r}
# bail's main outcome measure is an index of issue attitudes
# we need to replicate the index
# invert questions that prime liberal values
twitter_data$government_should_regulate_businesses_wave_1 <- 8 - twitter_data$government_should_regulate_businesses_wave_1
twitter_data$racial_discrimination_hurts_black_people_wave_1 <- 8 - twitter_data$racial_discrimination_hurts_black_people_wave_1
twitter_data$immigrants_strengthen_country_wave_1 <- 8 - twitter_data$immigrants_strengthen_country_wave_1
twitter_data$corporations_make_too_much_profit_wave_1 <- 8 - twitter_data$corporations_make_too_much_profit_wave_1
twitter_data$homosexuality_should_be_accepted_wave_1 <- 8 - twitter_data$homosexuality_should_be_accepted_wave_1
twitter_data$government_should_regulate_businesses_wave_5 <- 8 - twitter_data$government_should_regulate_businesses_wave_5
twitter_data$racial_discrimination_hurts_black_people_wave_5 <- 8 - twitter_data$racial_discrimination_hurts_black_people_wave_5
twitter_data$immigrants_strengthen_country_wave_5 <- 8 - twitter_data$immigrants_strengthen_country_wave_5
twitter_data$corporations_make_too_much_profit_wave_5 <- 8 - twitter_data$corporations_make_too_much_profit_wave_5
twitter_data$homosexuality_should_be_accepted_wave_5 <- 8 - twitter_data$homosexuality_should_be_accepted_wave_5
twitter_data$substantive_ideology_scale_wave_1 <- rowMeans(twitter_data[, c("government_should_regulate_businesses_wave_1", "racial_discrimination_hurts_black_people_wave_1", "immigrants_strengthen_country_wave_1", "corporations_make_too_much_profit_wave_1", "homosexuality_should_be_accepted_wave_1", "government_wasteful_inefficient_wave_1", "poor_people_have_it_easy_wave_1", "government_cannot_afford_to_help_needy_wave_1", "best_way_peace_military_strength_wave_1", "stricter_environmental_laws_damaging_wave_1")], na.rm = TRUE)
twitter_data$substantive_ideology_scale_wave_5 <- rowMeans(twitter_data[, c(
    "government_should_regulate_businesses_wave_5", "racial_discrimination_hurts_black_people_wave_5", "immigrants_strengthen_country_wave_5",
    "corporations_make_too_much_profit_wave_5",
    "homosexuality_should_be_accepted_wave_5",
    "government_wasteful_inefficient_wave_5",
    "poor_people_have_it_easy_wave_5",
    "government_cannot_afford_to_help_needy_wave_5",
    "best_way_peace_military_strength_wave_5",
    "stricter_environmental_laws_damaging_wave_5"
)], na.rm = TRUE)

# now look at the correlations between this and controls
twitter_data$ideology_norm <- (twitter_data$substantive_ideology_scale_wave_1 - mean(twitter_data$substantive_ideology_scale_wave_1, na.rm = TRUE)) / sd(twitter_data$substantive_ideology_scale_wave_1, na.rm = TRUE)
model <- lm(data = twitter_data, ideology_norm ~ gender + party_id_wave_1)
summary(model)
```

```{r}
# power simulation using control variables
possible.ns <- seq(from = 100, to = 1200, by = 10) # The sample sizes we'll be considering

stopifnot(all((possible.ns %% 2) == 0)) ## require even number of experimental pool
powers <- rep(NA, length(possible.ns)) # Empty object to collect simulation estimates
alpha <- 0.05 # Standard significance level
sims <- 400 # Number of simulations to conduct for each N
#### Outer loop to vary the number of subjects ####
for (j in 1:length(possible.ns)) {
    N <- possible.ns[j] # Pick the jth value for N
    significant.experiments <- rep(NA, sims) # Empty object to count significant experiments

    #### Inner loop to conduct experiments "sims" times over for each N ####
    population <- sample_n(twitter_data, N)
    sd <- sqrt(deviance(model) / df.residual(model))
    Y0 <- predict(model, population)
    noise <- rnorm(N, 0, sd = sd)
    Y0 <- Y0 + noise
    tau <- 0.1 # Hypothesized treatment effect
    Y1 <- Y0 + tau # treatment potential outcome
    for (i in 1:sims) {
        population$Z.sim <- sample(rep(c(0, 1), N / 2)) ## Do a random assignment ensuring equal sized groups
        population$Y.sim <- Y1 * population$Z.sim + Y0 * (1 - population$Z.sim) # Reveal outcomes according to assignment
        fit.sim <- lm(data = population, Y.sim ~ Z.sim + gender + party_id_wave_1) # Do analysis (Simple regression)
        p.value <- summary(fit.sim)$coefficients[2, 4] # Extract p-values
        significant.experiments[i] <- (p.value <= alpha) # Determine significance according to p <= 0.05
    }
    powers[j] <- mean(significant.experiments) # store average success rate (power) for each N
}
df <- data.frame(powers = powers, ns = possible.ns)
ggplot(data = df, aes(x = ns, y = powers)) +
    geom_point()
```

Using the bail replication data, we can expect N=1200 to get us a lot closer to $\beta=0.8$, a much better improvement over 3K participants